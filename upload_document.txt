Complete this document and upload along with your prediction results and your code.

### Method Name ###
Use a two-part name to describe your method, e.g. bag-of-words + FFNN, cipherword word2vec + BiLSTM, etc.
Answer: pretrained DistilBERT custom embeddings + pretrained DistilBERT.
### Representation of sentence ###
Use up to 3 sentences to describe how you obtained the representation/features of each ciphertext sentence. E.g., bag-of-words? trained a word2vec or fasttext on all sentences from scratch?
Answer: I obtained the features using DistilBERT's embeddings. DistilBERT comprises three types of embeddings: token embeddings, segment embeddings, and position embeddings. The Tokenizer first converts the input text into a list of token IDs, and the embedding layer is responsible for computing a final embedding for each input token by combining the three types of embeddings.
### Classifier ###
Use up to 5 sentences to describe how you implemented your classifier. What encoder did you use and what was the learning objective?
Answer: For my classifier, I fine-tuned a pretrained DistilBERT model. DistilBERT is a distilled version of BERT, providing comparable performance with reduced computational resources. DistilBERT serves as the encoder, leveraging its pretrained weights to capture contextual information from input text efficiently. 
The learning objective is to adapt DistilBERT's representations to the certain task of classification, which is cyphertext classification in this assignment. Through fine-tuning DistilBERT, we can leverage its pretrained knowledge of language and tailor it to our specific cyphertext classification task.
### Training & Development ###
Up to 5 sentences: how did you evaluate your solution using the dev set before submitting? What are some key hyperparameter values (e.g., optimizer, learning rate, batch size, etc.)? How did you terminate the training (using a fixed #epochs, early stopping based on dev set performance)?
Answer: I evaluate the model by computing the accuracy of predictions and loss on the dev set. Additionally, the plot visualizes the validation accuracy, training loss, and validation loss, which reflect the fact that the model is learning and not overfitting, as indicated by the increasing trend in validation accuracy.
For hyperparameters, I select Adam as the optimizer with an initial learning rate of 2e-5, a batch size of 16, and run training for 10 epochs.
To determine when to stop training the model, I set a fixed epoch limit of 10 and also include an early stopping callback that monitors the dev accuracy. The training process will terminate if the model validation accuracy on the dev set does not improve by at least 1e-6 over 3 consecutive epochs.
### Other methods ###
Did you try other methods than the submitted one?
Answer: Yes, I have tried two other methods: Word2vec + BiLSTM and FastText + BiLSTM. I found that compared to the BERT pretrained model, the training time is much shorter, taking only nearly 5 minutes, while it takes me 5 hours to fine-tune BERT. 
Surprisingly, the accuracy using BiLSTM is quite good, hitting around 80% and 81% with Word2vec and FastText respectively on the dev data. I tried two embeddings, Word2vec and FastText, and the results show little difference: FastText + BiLSTM has slightly better accuracy over Word2vec + BiLSTM.
### Packages ###
List the key python packages you have used in this assignment.
Answer: pandas, transformers, datasets, evaluate, numpy, tensorflow, matplotlib.
